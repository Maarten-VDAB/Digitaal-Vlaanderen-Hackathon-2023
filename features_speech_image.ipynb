{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git@v20230918\n",
    "%pip install openai~=0.28.1\n",
    "%pip install elevenlabs==0.2.21\n",
    "%pip install gradio~=3.41.0\n",
    "# Install `llava` to interact with LLaVA 1.5.\n",
    "%pip install -qq git+https://github.com/haotian-liu/LLaVA.git@v1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define load model and load image functions.\n",
    "import urllib.request\n",
    "from functools import lru_cache\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from PIL import Image\n",
    "\n",
    "@lru_cache\n",
    "def load_llava_model(model_path=\"radix-ai/llava-v1.5-7b\"):\n",
    "    # The original liuhaotian/llava-v1.5-7b has a shard size of 10GB which\n",
    "    # causes an out of memory error on Colab [1]. To fix this, we uploaded the\n",
    "    # model with 2GB shards to radix-ai/llava-v1.5-7b. Larger versions of this\n",
    "    # model are also available [2].\n",
    "    #\n",
    "    # [1] https://github.com/haotian-liu/LLaVA/issues/496\n",
    "    # [2] https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md.\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=model_path.split(\"/\")[-1],\n",
    "        load_8bit=True,  # Quantize to 8 bit to fit on Google Colab's T4 GPU.\n",
    "        load_4bit=False\n",
    "    )\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "@lru_cache\n",
    "def load_image(image_url):\n",
    "    # image_filename, _ = urllib.request.urlretrieve(image_url)\n",
    "    image = Image.open(image_url).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "llava_model = load_llava_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image.\n",
    "# image = load_image(\"https://i.imgur.com/gFmBBCw.jpg\")\n",
    "print(type(\"monopoly.jpeg\"))\n",
    "image = load_image(\"monopoly.jpeg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inference function, based on `llava.serve.cli`.\n",
    "import torch\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava.utils import disable_torch_init\n",
    "from transformers import TextStreamer\n",
    "\n",
    "def ask_question(model, image, question):\n",
    "    # Unpack model.\n",
    "    tokenizer, model, image_processor, context_len = model\n",
    "    disable_torch_init()\n",
    "    # Convert image.\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
    "    # Generate prompt.\n",
    "    conv_mode = \"llava_v1\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    roles = conv.roles\n",
    "    inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + question\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    # Inference.\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            streamer=streamer,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria]\n",
    "        )\n",
    "    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "elevenlabs_pw = getpass.getpass(prompt=\"ElevenLabs API key: \")\n",
    "openai_pw = getpass.getpass(prompt=\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with ElevenLabs and OpenAI.\n",
    "import openai\n",
    "from elevenlabs import set_api_key\n",
    "set_api_key(elevenlabs_pw)\n",
    "openai.api_key = openai_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model.\n",
    "import whisper\n",
    "whisper_model = whisper.load_model(\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make chatbot with speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make chatbot + add speech feature\n",
    "import gradio as gr\n",
    "from elevenlabs import generate, save\n",
    "\n",
    "def add_text(chat_history, text_input):\n",
    "    # Add a user message to the chat history.\n",
    "    chat_history += [(text_input, None)] # text_input = User message, None = chatbot message\n",
    "    return chat_history, \"\"\n",
    "\n",
    "def add_image(chat_history, image_input):\n",
    "    # Add an image to the chat history.\n",
    "    #TODO: save image somewhere to load it into the \"load_image\" function\n",
    "    name = image_input.name.split()\n",
    "    file_name = name[-1]\n",
    "    image = load_image(file_name)\n",
    "    question = \"Please analyse what this image is about. Do not return any personal information contained in the image.\"\n",
    "    response = ask_question(llava_model, image, question)\n",
    "    chat_history += [((image_input.name,), response)]\n",
    "    yield chat_history\n",
    "\n",
    "def add_audio(chat_history, audio_input):\n",
    "    # Convert audio input file to transcription.\n",
    "    audio = whisper.load_audio(audio_input)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "    transcription = whisper.decode(whisper_model, mel)\n",
    "    chat_history += [(transcription.text, None)]\n",
    "    return chat_history\n",
    "\n",
    "def add_llm_response(chat_history):\n",
    "    # Convert chat_history to OpenAI's format.\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant\"})\n",
    "    for pair in chat_history:\n",
    "        if isinstance(pair[0], str):\n",
    "            messages.append({\"role\": \"user\", \"content\": pair[0]})\n",
    "        if isinstance(pair[1], str):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": pair[1]})\n",
    "    # Request streaming response from GPT.\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    # Stream response to chat history as it arrives.\n",
    "    for chunk in response:\n",
    "        if \"content\" in chunk[\"choices\"][0][\"delta\"]:\n",
    "            if not chat_history[-1][1]:\n",
    "                chat_history[-1][1] = \"\"\n",
    "            chat_history[-1][1] += chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
    "        yield chat_history\n",
    "\n",
    "def play_llm_response(chat_history):\n",
    "    # Generate speech from the LLM response.\n",
    "    audio_filepath = \"llm_response.wav\"\n",
    "    audio = generate(\n",
    "        text=chat_history[-1][1],\n",
    "        voice=\"Grace\",\n",
    "        model=\"eleven_multilingual_v2\"\n",
    "    )\n",
    "    save(audio, audio_filepath)\n",
    "    return audio_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo app.\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    # Create widgets.\n",
    "    chat_history = gr.Chatbot([], avatar_images=(\"VDAB_logo_donkerblauw_RGB.jpg\", \"monopoly.jpeg\"))\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(scale=2, placeholder=\"‚úçÔ∏è Enter message\", container=False)\n",
    "        image_input = gr.UploadButton(\"üì∑ Upload image\", file_types=[\"image\"])\n",
    "        audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", container=False)\n",
    "        audio_output = gr.Audio(type=\"filepath\", autoplay=True, visible=False)\n",
    "\n",
    "    # Link widget events to Python functions.\n",
    "    text_input.submit(add_text, [chat_history, text_input], [chat_history, text_input], queue=False).then(\n",
    "        add_llm_response, chat_history, chat_history).then(\n",
    "        play_llm_response, chat_history, audio_output, queue=False)\n",
    "    image_input.upload(add_image, [chat_history, image_input], [chat_history], queue=False).then(\n",
    "        play_llm_response, chat_history, audio_output, queue=False)\n",
    "    audio_input.stop_recording(add_audio, [chat_history, audio_input], [chat_history], queue=False).then(\n",
    "        add_llm_response, chat_history, chat_history).then(\n",
    "        lambda: None, None, audio_input, queue=False).then(\n",
    "        play_llm_response, chat_history, audio_output, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
